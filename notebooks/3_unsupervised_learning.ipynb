{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"top\"></a>\n",
    "# Table of Contents\n",
    "- [3. Unsupervised Learning](#3)\n",
    "    - [3.1 Unsupervised vs supervised learning](#3_1)\n",
    "    - [3.2 Introductory examples](#3_2)\n",
    "    - [3.3 Unsupervised Learning approaches](#3_3)\n",
    "        - [3.3.1 Clustering](#3_3_1)\n",
    "            - [K-Means Clustering](#3_3_1_1)\n",
    "            - [Hierarchical Clustering](#3_3_1_2)\n",
    "            - [Difference between K-Means and Hierarchical clustering](#3_3_1_3)\n",
    "        - [3.3.2 Dimensionality Reduction](#3_3_2)\n",
    "            - [PCA](#3_3_2_1)\n",
    "            - [t-SNE](#3_3_2_2)\n",
    "    - [3.4 Advantages and Challenges](#3_4)\n",
    "    - [3.5 Exercises](#3_5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Unsupervised Learning<a name=\"3\"></a>\n",
    "\n",
    "The most common tasks within unsupervised learning are clustering, dimension reduction, and density estimation. In all of these cases, we wish to learn the inherent structure of our data without using explicitly-provided labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Unsupervised vs. supervised learning<a name=\"3_1\"></a>\n",
    "\n",
    "In comparison to supervised learning, in the setting of unsupervised learning we observe observations with feature vectors\n",
    "$x_i = (x_{i,1},\\dots,x_{i,n})$ for $i=1,\\dots,p$\n",
    "but no associated labels $y_i$ - the response variable is missing.\n",
    "<img alt=\"AI\" src=\"../img/3/schema.jpg\" width=\"400\" />\n",
    "Nevertheless we want to gain insight in the **relationship between the variables** itself as well as **between the observations**.\n",
    "Generally speaking we want to understand the underlying hidden structures within the observed data.\n",
    "\n",
    "In unsupervised learning, attempts are made to **detect patterns** directly from the available example as no labelled datasets are provided. \n",
    "\n",
    "<img src=\"../img/3/supvsunsup.png\" alt=\"Supervised_vs_Unsupervised\" width=\"450\" />\n",
    "\n",
    "Source: [beta.cambridgespark.com](http://beta.cambridgespark.com/courses/jpm/01-module.html)\n",
    "\n",
    "The left plot is an example of supervised learning; regression techniques are used to find the line of best fit between the features. In unsupervised learning, the inputs are segregated based on features and the prediction is based on which cluster it belongs to.\n",
    "\n",
    "From a probabilistic point of view, supervised learning tries to infer a conditional probability distribution $P_X(x \\mid y)$ whereas unsupervised learning intends to infer an a priori distribution $P_X(x)$.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 3.2. Introductory examples<a name=\"3_2\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Example 1: Density estimation\n",
    "Estimation of probability density functions is a central application of unsupervised learning, e.g. histograms and kernel density estimators:\n",
    "<img src=\"../img/3/density_estimation.png\" width=\"400\" />\n",
    "\n",
    "Source: Yen-Chi Chen, STAT 425: Introduction to Nonparametric Statistics, Winter 2018, chapter 6.1, University of Washington\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Example 2\n",
    "\n",
    "- **Supervised setup:**\n",
    "You get a bunch of photos of animals with information about what species is on them and then you train a model to recognize new photos.\n",
    "- **Unsupervised setup:**\n",
    "You have a bunch of photos of 4 species but without information about which species is on which one and you want to divide this dataset into 4 piles, each with the photos of one species.\n",
    "\n",
    "\n",
    "\n",
    "<img alt=\"Customer Segmentation\" src=\"../img/3/unsupervisedexample.png\" width=\"500\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Applications of unsupervised learning\n",
    "\n",
    "- Market segmentation: An advertising platform **segments the population into smaller groups** which similar characteristics e.g. similar demographics, lifestyles, and purchasing habits so that advertisers can reach their target market with relevant ads. This helps advertisers to understand their existing customer base and use their ad spend effectively by targeting potential new customers.\n",
    "\n",
    "<img alt=\"Customer Segmentation\" src=\"../img/3/customer-segmentation.jpg\" width=\"400\" />\n",
    "\n",
    "Source: [www.analyticsvidhya.com](https://www.analyticsvidhya.com/blog/2018/05/essentials-of-deep-learning-trudging-into-unsupervised-deep-learning/)\n",
    "\n",
    "- E.g. segmentation exchange members\n",
    "-  **Association**: Finding correlated features in order to infer other unknown characteristics\n",
    "- Companies like Airbnb group housing listings in order to make better recommendations \n",
    "- **Reducing dimensionality** of datasets in order to support visual inspection and/or reduce computational costs.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 3.3. Unsupervised Learning approaches<a name=\"3_3\"></a>\n",
    "* **Clustering** (Segmentation): Splitting the dataset into groups according to similarity\n",
    "* **Dimensionality Reduction** (Compression)\n",
    "<img alt=\"Customer Segmentation\" src=\"../img/3/unsupervisedvssupervised.jpg\" width=\"400\" />\n",
    "\n",
    "\n",
    "\n",
    "* Anomaly Detection: Discover unusual data points in your dataset e.g. Fraud detection\n",
    "* Association mining: Infer unknown characteristics of items within one cluster\n",
    "* Other algorithms: Autoencoders, Deep Belief Nets, Generative Adversarial Networks (GANs), Self-Organizing maps\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### 3.3.1 Clustering<a name=\"3_3_1\"></a>\n",
    "\n",
    "In clustering, we are trying to partition the data into several distinct **subgroups**, also called clusters, according to similar features. Items within a cluster are more similar to each other than items in other clusters. It's up to us to interpret the results: value is created when humans create meaning using the findings.\n",
    "\n",
    "Recall the customer segmentation example. Customers can be divided into groups according to certain features (e.g. age, gender, zip code, family income, user history).\n",
    "Clustering algorithms can find natural groupings in data, if they exist.\n",
    "\n",
    "<img src=\"../img/3/clusters.jpeg\" alt=\"clusters\"/>\n",
    "\n",
    "Source: https://medium.com/the-21st-century/machine-learning-a-strategy-to-learn-and-understand-chapter-3-9daaad4afc55\n",
    "\n",
    "The left image shows raw data; the right plot shows clustered data. A new input is classified into one of these clusters based on features and a prediction is made.\n",
    "\n",
    "There are several clustering algorithms. We'll explore two of the most common ones: **k-Means clustering** and **Hierarchical clustering**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate some data:  Make your own random clusters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# create blobs\n",
    "n_true_clusters = 4\n",
    "data = make_blobs(n_samples=200, n_features=2, centers=n_true_clusters, cluster_std=1.1)\n",
    "\n",
    "X, y = data[0],  data[1]\n",
    "\n",
    "# create scatter plot\n",
    "plt.scatter(X[:,0], X[:,1], c=y, edgecolor=None)\n",
    "plt.xlabel(\"$X_1$\")\n",
    "plt.ylabel(\"$X_2$\")\n",
    "plt.title(\"Play dataset with {} clusters\".format(n_true_clusters))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "#### 3.3.1.1 K-Means Clustering <a name=\"3_3_1_1\"></a>\n",
    "\n",
    "K-Means divides data points into **K mutually exclusive clusters**. Choosing the desired number for K can be difficult. A larger K creates smaller groups with more granularity, a smaller K results larger groups and less granularity.\n",
    "If we know that there are 2 classes involved, we tell the algorithm to find 2 clusters. \n",
    "\n",
    "##### Objective:\n",
    "* Let $C_1,\\dots, C_K$ denote sets containing the indices of the observations.\n",
    "* Let $W(C_k)$ be a measure of how different data points are within cluster $C_k$\n",
    "* Minimize within-cluster variation:\n",
    "$$minimize_{C_1,\\dots,C_K} \\Big\\{ \\sum_{k=1}^{K} W(C_k)\\Big\\}$$\n",
    "\n",
    "* Using squared Euclidean norm as distance measure:\n",
    "$$W(C_k) = \\frac{1}{ |C_k|} \\sum_{i,i' \\in C_k}  \\sum_{j=1} ^{p} (x_{i,j}-x_{i',j})^2$$\n",
    "\n",
    "* Difficult to solve optimization problem but the following simple algorithm provides a local optimum. \n",
    "\n",
    "##### Algorithm:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td width=30% style=\"text-align:left; LINE-HEIGHT:200%\">\n",
    "            Steps in k-means clustering:\n",
    "            <ol>\n",
    "              <li>Initialize k centroids at random.</li>\n",
    "              <li>Assign each data point to one of the k clusters. Measure of \"nearness\" is a hyperparameter — often Euclidean distance.</li>\n",
    "              <li>Move the centroids to the center of their respective clusters. The new position of each centroid is calculated as the average position of all the points in its cluster.</li>\n",
    "                <li>Keep repeating steps 2 and 3 until the centroids stop moving a lot at each iteration, until the algorithm converges.</li>\n",
    "            </ol>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"../img/3/kmeans_algo.png\" />\n",
    "            modified after this <A href=\"http://www.youtube.com/watch?v=_aWzGGNrcic\">youtube video</A>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "##### Algorithm (slightly different initialization step)\n",
    "<img src=\"../img/3/kmeans_algo2.png\" />\n",
    "\n",
    "Source: An Introduction to Statistical Learning, Trevor Hastie, Robert Tibshirani\n",
    "\n",
    "* Since the algorithm gives only a local optimum rather than a global one, the obtained result depends on initial randomly assigned cluster centroids.\n",
    "\n",
    "\n",
    "* One application of k-means clustering is **classifying handwritten digits**. Suppose we have digits' images as vectors of pixel brightnesses. Let's say the images are black and white with 64x64 pixels. Each pixel represents a dimension. k-means clustering allows us to group the images that are close together, achieving good results for digit recognition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Modules\n",
    "from sklearn.cluster import KMeans\n",
    "import random\n",
    "\n",
    "K=4\n",
    "# Declaring Model\n",
    "model = KMeans(n_clusters=K)\n",
    "\n",
    "# Fitting Model\n",
    "model.fit(X)\n",
    "\n",
    "# Prediction on the entire data\n",
    "y_predict = model.predict(X)\n",
    "\n",
    "\n",
    "# make lists of colors for plotting\n",
    "random.seed(43)\n",
    "def random_color_list(length=10):\n",
    "    return [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]) for i in range(length)]\n",
    "\n",
    "random_colors_y = random_color_list(len(set(y)))\n",
    "random_colors_p = random_color_list(len(set(y_predict)))\n",
    "\n",
    "plt.title(\"K-Means clustering K={}, true clusters={}\".format(K, n_true_clusters))\n",
    "for xi, yi, cy, cp in zip(X[:,0], X[:,1], y, y_predict):\n",
    "    plt.scatter(xi, yi, c=random_colors_p[cp], edgecolors=None , linewidths=0, alpha=0.8, s=80)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 3.3.1.2 Hierarchical Clustering<a name=\"3_3_1_2\"></a>\n",
    "\n",
    "Alternative approach which doe snot require to define K, the number of clusters, beforehand. In fact hierachical clustering provides you with tree-based hierarchical structur of sub groups, called **dentrogram**.\n",
    "\n",
    "1. **Bottom-up** or agglomerative clustering. \n",
    "    - This is the most common type of hierarchical clustering, \n",
    "    - It is agglomerative (a dendrogram is built starting from the leaves)\n",
    "2. **Top-down** or divisive clustering. \n",
    "    - It is divisive (a dendrogram is built starting from the trunk)\n",
    "\n",
    "<img src=\"../img/3/hclust.jpg\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bottom Up\n",
    "All the data points are assigned to individual clusters. Then the 2 closest clusters are joined into one. The algorithm ends when there is only one single cluster left. The completion of hierarchical clustering is shown using **dendrograms**. \n",
    "\n",
    "It clusters data points into **parent** and **child clusters**. We can divide our customers into younger and older ages, and then split each of those groups into further individual clusters as well. In addition to cluster assignments, we also build a nice tree that tells us about the hierarchies between the clusters. We can then pick the number of clusters we want from this tree.\n",
    "\n",
    "Hierarchical clustering is similar to regular clustering, except that you build a hierarchy of clusters. This is useful if you want flexibility in how many clusters you ultimately want.\n",
    "\n",
    "Here are the steps for hierarchical clustering:\n",
    "1. Start with N clusters, one for each data point.\n",
    "2. Merge the two clusters that are closest to each other. Now you have N-1 clusters.\n",
    "3. Recompute the distances between the clusters. There are several ways to do this.\n",
    "4. Repeat steps 2 and 3 until you get one cluster of N data points. You get a tree (also known as a dendrogram) like the one below.\n",
    "5. Pick a number of clusters and draw a horizontal line in the dendrogram.\n",
    "\n",
    "Two open issues:\n",
    "  1. What means close?\n",
    "  2. How do measure the distance between clusters?\n",
    "  \n",
    "**To 1:** Define **dissimilarity measure** to measure distance between two data points; most often Euclidean distance.\n",
    "  \n",
    "**To 2:** Extend the definition of dissimilary measure to group of data points: linkage-types:\n",
    "\n",
    "<img src=\"../img/3/linkagetypes.png\" width=\"500\" />\n",
    "\n",
    "* The resulting dendrogram depends strongly on the choice of the dissimilarity measure and linkage.\n",
    "* E.g. correlation-based distance for clutering customers based on their shopping history\n",
    "* Consider scaling of data points (mean zero, standard deviation 1)\n",
    "* \"With these methods, there is no\n",
    "single right answer—any solution that exposes some interesting aspects of\n",
    "the data should be considered.\" (Trevor Hastie, Robert Tibshirani)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example of hierarchical clustering of grain data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Importing Python Modules\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "# Reading the DataFrame\n",
    "# \"https://raw.githubusercontent.com/vihar/unsupervised-learning-with-python/master/seeds-less-rows.csv\"\n",
    "\n",
    "seeds_df = pd.read_csv(\"../datasets/3/seeds-less-rows.csv\")\n",
    "\n",
    "\n",
    "# Remove the grain species from the DataFrame, save for later\n",
    "varieties = list(seeds_df.pop('grain_variety'))\n",
    "\n",
    "# Extract the measurements as a NumPy array\n",
    "samples = seeds_df.values\n",
    "\n",
    "\"\"\"\n",
    "Perform hierarchical clustering on samples using the\n",
    "linkage() function with the method='complete' keyword argument.\n",
    "Assign the result to mergings.\n",
    "\"\"\"\n",
    "mergings = linkage(samples, method='complete')\n",
    "\n",
    "\"\"\"\n",
    "Plot a dendrogram using the dendrogram() function on mergings,\n",
    "specifying the keyword arguments labels=varieties, leaf_rotation=90,\n",
    "and leaf_font_size=6.\n",
    "\"\"\"\n",
    "dendrogram(mergings,\n",
    "           labels=varieties,\n",
    "           leaf_rotation=90,\n",
    "           leaf_font_size=8,\n",
    "           )\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 3.3.1.3 Difference between K Means and Hierarchical clustering<a name=\"3_3_1_3\"></a>\n",
    "\n",
    "* Hierarchical clustering can't handle **big data** well but K means clustering can. This is because the time complexity of K means is linear i.e. $O(n)$ while that of hierarchical clustering is quadratic i.e. $O(n^2)$.\n",
    "\n",
    "* Results are **reproducible** in hierarchical clustering. In K means clustering, as we start with an arbitrary choice of clusters, the results generated by running the algorithm multiple times might differ. \n",
    "\n",
    "* K means is found to work well when the **shape of the clusters** is hyper spherical (like circle in 2D, sphere in 3D).\n",
    "\n",
    "* K-Means doesn't allow **noisy data**, while in hierarchical we can directly use noisy dataset for clustering.\n",
    "\n",
    "* Hierarchical clustering tends to produce **more accurate** results compared to k-means clustering. The downside is that hierarchical clustering is more difficult to implement and more time/resource consuming than k-means.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 3.3.2 Dimensionality Reduction<a name=\"3_3_2\"></a>\n",
    "\n",
    "Running algorithms on only data that's necessary is sensible. Unsupervised learning can help with dimensionality reduction and reduce data complexity while maintaining data structure and usefulness. Reducing dimensionality of data is an important part of a good machine learning pipeline and resembles compression. In computer vision, reducing the size of training set by an order of magnitude by running algorithms on images would significantly lower compute and storage costs while making models run faster. \n",
    "\n",
    "Dimensionality reduction (dimensions or how many columns are in your dataset) assumes that a lot of data is redundant (meaning being highly correlated), and that most of the information in a dataset can be fairly well approximated by only a fraction. This means combining parts of the data to convey maximum information. \n",
    "\n",
    "- Principal Component Analysis (PCA): finds the linear combinations that conveys most of the variance in your data.\n",
    "- Singular-Value Decomposition (SVD): decomposes data into product of smaller matrices.\n",
    "\n",
    "These methods use linear algebra to break down a matrix into more informatory pieces.\n",
    "\n",
    "We'll take a look at one common technique in practice: Principal component analysis (PCA).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2.1 Principal Components Analysis<a name=\"3_3_2_1\"></a>\n",
    "\n",
    "\n",
    "PCA is a statistical procedure that uses a transformation to convert a set of observations of possibly correlated variables into a set of values of **linearly uncorrelated variables**, that explains most of the variability of the original set, called **principal components**.\n",
    "\n",
    "This transformation is defined in such a way that the **first principal component** has the *largest possible variance* , and **each succeeding component** in turn has the *highest variance possible* under the constraint that it is orthogonal to the preceding components. \n",
    "<A href=\"https://en.wikipedia.org/wiki/Principal_component_analysis\">wikipedia PCA</A>\n",
    "\n",
    "**Goal:** Find a low-dimensional representation of the data which captures most of the information.\n",
    "\n",
    "* Let's consider $n$ observation from a $p$-dimensional space\n",
    "* Let $X_1, \\dots, X_p$ be the the set of features.\n",
    "* Define the **first principal component $Z_1$** as the normilized linear combination\n",
    "$$Z_1 = \\sigma_{1,1} X_1 + \\sigma_{2,1} X_2 + \\dots + \\sigma_{p,1} X_p$$\n",
    "with\n",
    "$$\\sum_{j=1}^p \\sigma_{j,1}^2 = 1$$\n",
    "* w.l.o.g. assume $X_1,\\dots,X_p$ have mean zero\n",
    "* $\\sigma_{1,1}, \\dots, \\sigma_{p,1}$ are called loadings of the first principal component\n",
    "* First principal compoment in vector-matrix notation:\n",
    "$$z_{i,1} = \\sigma_{1,1}x_{i,1} + \\sigma_{2,1}x_{i,2} + \\dots +  \\sigma_{p,1}x_{i,p} $$\n",
    "* Recalling that the first compoment should have the largest variance, determining the first principal component becomes the following optimization problem:\n",
    "$$maximize_{\\sigma_{1,1},\\dots,\\sigma_{p,1}} \\Big\\{  \\frac{1}{n} \\sum_{i=1}^n \\Big(  \\sum_{j=1}^p\\sigma_{j,1}x_{i,j}  \\Big)^2  \\Big\\} \\qquad \\text{ subject to } \\qquad \\sum_{j=1}^p \\sigma_{j,1}^2 = 1$$\n",
    "* After rearrangement this can be solved via an eigen decomposition (here out of scope)\n",
    "* This maps the input data from an $nxp$ space to an new $nxp$ space.\n",
    "\n",
    "The subset of principal components we select will constitute a new space that is smaller in dimensionality than the original space but maintains as much of the complexity of the data as possible. PCA remaps the space in which our data exists.\n",
    "\n",
    "<img src=\"../img/3/pca_2comp.png\" width=\"500\" alt=\"pca_2comp\" />\n",
    "\n",
    "The loading vector $\\sigma_1$ of the first principal component defines the direction in the feature space along which the data vary the most. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Example \n",
    "The original Iris dataset contains data about 3 spacies of iris (setosa, versicolor,virginica). It has 5 columns: sepal length, sepal width, petal length, petal width nd species.\n",
    "    \n",
    "<img src=\"../img/3/Iris_dataset_scatterplot.png\" width=\"500\" alt=\"\" />\n",
    "\n",
    "The following code projects the original data which is 4 dimensional (${x_1,x_2,x_3,x_4}$) into 2 dimensions (${z_1,z_2}$). After dimensionality reduction, there isn’t a particular meaning assigned to each principal component typically.\n",
    "The new components are just the 2 main dimensions of variation. \n",
    "\n",
    "After plotting the data in the new dimensions(${z_1,z_2}$), the different classes seem well separated from each other.\n",
    "\n",
    "The **explained variance**  tells us how much information (variance) can be attributed to each of the principal components. This is important as converting 4 dimensional space to 2 dimensional space can lose some of the variance (information). Here, the first principal component contains 92.46% of the variance and the second principal component contains 5.30% of the variance. Together, the two components contain 97.76% of the total information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline\n",
    "# Load the iris data\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_scaled = min_max_scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "# Fit a PCA\n",
    "pca = PCA(n_components=2, whiten=True)\n",
    "pca.fit(X)\n",
    "\n",
    "# Percentage of variance explained for each components\n",
    "print('explained variance ratio (first two components): %s'\n",
    "      % str(pca.explained_variance_ratio_))\n",
    "\n",
    "# Project the data in 2D\n",
    "X_pca = pca.transform(X_scaled)\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure()\n",
    "lw = 2 \n",
    "target_ids = range(len(iris.target_names))\n",
    "plt.figure(figsize=(6, 5))\n",
    "for i, c, label in zip(target_ids, 'rgbcmykw', iris.target_names):\n",
    "    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1],\n",
    "               c=c, label=label)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('PCA of IRIS dataset')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2.2  t-SNE<a name=\"3_3_2_2\"></a>\n",
    "\n",
    "t-SNE (t-distributed stochastic neighbor embedding) is one of the unsupervised learning methods for visualisation. It maps high dimensional space into a 2 or 3 dimensional space which can be visualised. Specifically, it models each high-dimensional object by a 2 or 3-dimensional point in such a way that similar objects are modelled by nearby points and dissimilar objects are modelled by distant points with high prob 2-dimensional figure. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Python Modules\n",
    "from sklearn import datasets\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Loading dataset\n",
    "iris_df = datasets.load_iris()\n",
    "\n",
    "# Defining Model\n",
    "model = TSNE(learning_rate=100, perplexity=12, n_components=2)\n",
    "\n",
    "# Fitting Model and transforming the data\n",
    "transformed = model.fit_transform(iris_df.data)\n",
    "\n",
    "# Plotting 2d t-Sne\n",
    "x_axis = transformed[:, 0]\n",
    "y_axis = transformed[:, 1]\n",
    "\n",
    "plt.scatter(x_axis, y_axis, c=iris_df.target)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 3.4 Advantages and Challenges<a name=\"3_4\"></a>\n",
    "\n",
    "###  3.4.1. Advantages\n",
    "    \n",
    "* Supervised learning is **constrained by the biases** in which it is being supervised in; it cannot think of other corner cases that could occur when solving the problem.\n",
    "\n",
    "* Also, huge manual **effort is required to create labels** in supervised learning. The less the number of labels created, less is the training that can be perform for your algorithm.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.4.2. Challenges \n",
    "\n",
    "Unsupervised learning presents a unique challenge: it's difficult to know if we're getting the right answers without an answer key. In supervised learning, metrics like precision and recall reveal how accurate our model is, and model parameters are tuned to improve accuracy scores. **With absence of labels in unsupervised learning, we can't objectively measure how accurate our algorithm is.** How do we know if K-Means found the right clusters or even the right number of clusters? \n",
    "\n",
    "If unsupervised learning is right for the job, depends on your business context. \"Performance\" is often subjective and domain-specific. There is no universally accepted mechanism for performance cross validtion. Unsupervised learning is often used as part of **exploratory data analysis**. In customer segmentation, clustering will only work well, if your customers actually do split into natural groups. One of the best but risky ways to test an unsupervised learning model is by implementing it in the real world and seeing what happens. Designing an A/B test, with and without the clusters our algorithm outputted, can be an effective way to see if it's useful information or totally incorrect.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 3.5 Exercises<a name=\"3_5\"></a>\n",
    "1. K Means Clustering <A href=\"#3_3_1\">(go up)</A>  documentation on K-Means <A href=\"https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\">K-Means doc</A>: \n",
    "    - Play around with these parameters: **true_clusters** and **K**.<br/> What happens if you guess the right number of clusters with **K = true_clusters** and **otherwise**?\n",
    "    - Change **cluster_std** when generating data. <br>This will make the clusters more dispersed and it will be more difficult for the model to separate them.\n",
    "    - K Means might give you **different results** each time you run it. <br>Try to observe this behaviour. How can you avoid this in your code? <br><br>\n",
    "2. t-SNE <a href=\"#3_3_2_2\">(go up)</a> documenation on t-SNE: <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html\"> sklearn doc</a>\n",
    "    - Play around with these parameters: **perplexity** and **learning rate**.  <br>\n",
    "    Find a combination of both that gives you the **best separation** of the iris species."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Learning Material<a name=\"3_6\"></a>\n",
    "- https://towardsdatascience.com/unsupervised-learning-with-python-173c51dc7f03\n",
    "- https://scikit-learn.org/stable/tutorial/statistical_inference/unsupervised_learning.html\n",
    "- https://www.analyticsvidhya.com/blog/2018/05/essentials-of-deep-learning-trudging-into-unsupervised-deep-learning/\n",
    "- https://www.coursera.org/lecture/machine-learning/unsupervised-learning-olRZo\n",
    "- https://www.datacamp.com/courses/unsupervised-learning-in-python\n",
    "- https://www.kaggle.com/sashr07/unsupervised-learning-tutorial\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
