{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Table of Contents\n",
    "- [2. Supervised Learning](#2)\n",
    "    - [2.1 Regression](#2_1)\n",
    "        - [2.1.1 Simple Linear Regression](#2_1_1)\n",
    "        - [2.1.2 Multiple Linear Regression](#2_1_2)\n",
    "    - [2.2 Classification](#2_2)\n",
    "        - [2.2.1 Logistic Regression](#2_2_1)\n",
    "        - [2.2.2 K-Nearest Neighbours (KNN)](#2_2_2)\n",
    "        - [2.2.3 Decision Trees](#2_2_3)\n",
    "        - [2.2.4 Support Vector Machines](#2_2_3)\n",
    "    - [2.3 Exercise](#2_3)\n",
    "    - [2.4 Learning Material](#2_4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Supervised Learning<a name=\"2\"></a>\n",
    "\n",
    "#### Regression vs Classification\n",
    "\n",
    "##### Repetition:\n",
    "Supervised learning problems can be sub-divided into regression and classification problems.\n",
    "\n",
    "1. **Regression** covers situations where **y is numerical**. \n",
    "    - Predicting the **value** of the DAX next week.\n",
    "    - Predicting the **price** of a given house based on various inputs.\n",
    "\n",
    "2. **Classification** covers situations where **y is categorical-nominal**\n",
    "    - Will the DAX be **up or down** next week?\n",
    "    - Is this email a **SPAM or not**?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Regression<a name=\"2_1\"></a>\n",
    "\n",
    "Regression analysis helps one understand how the typical value of the dependent variable (y) changes when any one of the independent variables (X) is varied, while the other independent variables are held fixed. [wikipedia](https://en.wikipedia.org/wiki/Regression_analysis)\n",
    "\n",
    "- Linear regression is a simple approach to supervised learning\n",
    "- Only **Simple Linear Regression** assumes a linear dependence of Y on $X_1, X_2, ... X_n$\n",
    "- Linear Regression is very useful (conceptually and practically)\n",
    "\n",
    "<IMG alt=\"advertising\" width=550 src=\"../img/2/advertise.png\">\n",
    "\n",
    "source: [ISLR](http://www-bcf.usc.edu/~gareth/ISL/)\n",
    "\n",
    "Questions linear regression can help us to answer:\n",
    "1. Is there a relationship between advertising budget and sales? \n",
    "2. How strong is the relationship between advertising budget and sales? \n",
    "3. Which factors (media) contribute to sales?\n",
    "4. How accurately can we estimate the effect of each medium on sales? \n",
    "5. How accurately can we predict future sales?\n",
    "6. Is the relationship linear?\n",
    "7. Is there synergy among the advertising media?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Simple Linear Regression<a name=\"2_1_1\"></a>\n",
    "Simple linear regression assumes there is a linear relationship between x and y.\n",
    "\n",
    "$$ y = \\beta_{0}  + \\beta_{1} x $$\n",
    "\n",
    "<img src=\"../img/2/simple_linear_slope.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression with scikit-learn\n",
    "-  Generate some random data for regression analysis\n",
    "-  not train test split, because we have the truth\n",
    "-  Create a model with scikit-learn\n",
    "-  fit the model\n",
    "-  predict\n",
    "-  plot linear regression predictions with the target function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# generate random data from a line adding some noise\n",
    "def generate_regr(n_samples, x_mean, x_std, m, b, y_noise):\n",
    "    x, y = list(), list()\n",
    "    for i in range(n_samples):\n",
    "        x0 = random.gauss(x_mean, x_std)\n",
    "        y0 = m * x0 + b + (random.gauss(0, y_noise))\n",
    "        x.append(x0)\n",
    "        y.append(y0)\n",
    "    return np.array([x]).transpose(), y\n",
    "\n",
    "m, b, noise = 10, 0, 25\n",
    "X, y = generate_regr(50, 0, 10, m, b, noise)\n",
    "\n",
    "# plot dataset\n",
    "pyplot.scatter(X,y)\n",
    "\n",
    "# ----------- create a model and fit it to the data ----------\n",
    "regr_model = LinearRegression()\n",
    "regr_model.fit(X, y)\n",
    "\n",
    "# regression line\n",
    "xl = np.linspace(min(X), max(X), 100)\n",
    "yl = regr_model.coef_[0] * xl + regr_model.intercept_\n",
    "\n",
    "# truth or target function\n",
    "yt= m * xl + b\n",
    "\n",
    "pyplot.plot(xl, yl, '-r', label='linear regression')\n",
    "pyplot.plot(xl, yt, '-b', label='truth (target function)')\n",
    "pyplot.legend(loc='upper left')\n",
    "pyplot.show()\n",
    "pyplot.close()\n",
    "\n",
    "print(\"slope:\")\n",
    "print(\"  true:\", m, \" model: \", regr_model.coef_[0])\n",
    "\n",
    "print(\"intercept:\")\n",
    "print(\"  true:\", b, \" model: \", regr_model.intercept_)\n",
    "\n",
    "print(\"R-squared: \", regr_model.score(X, y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost Function\n",
    "\n",
    "The optimization algorithm tries to find a line for which the **distance of the data points (residuals)** to this line is minimized. The way to *'punish'* this distance is called a **cost function**.\n",
    "\n",
    "<img src=../img/2/residuals.png alt=\"residuals\" width=450>\n",
    "\n",
    "source: [ISLR](http://www-bcf.usc.edu/~gareth/ISL/)\n",
    "\n",
    "\n",
    "There are different **cost funtions** to pick from \n",
    "- RSS: Residual Sum of Squares\n",
    "$$  \\sum_i (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "- MAE: Mean Absolut Error\n",
    "$$ \\frac{1}{n} \\sum_i |y_i - \\hat{y}_i| $$\n",
    "is the easiest to understand, because it's the average error.\n",
    "\n",
    "\n",
    "- MSE: Mean Squared Error\n",
    "$$ \\frac{1}{n} \\sum_i (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "- RMSE Rooted MSE\n",
    "$$ \\sqrt{MSE}$$\n",
    "is even more popular than MSE, because RMSE is interpretable in the \"y\" units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Estimating Coefficients\n",
    "\n",
    "The algorithm goes through different combintions of the **coefficients ($ \\beta_{0} ,  \\beta_{1}  $)** and picks the one with the lowest cost.\n",
    "\n",
    "$$ y = \\beta_{0}  + \\beta_{1} x $$\n",
    "\n",
    "\n",
    "<IMG SRC=\"../img/2/coef_estimation.png\" width=500 alt=\"estimating coefficients\">\n",
    "\n",
    "source: [ISLR](http://www-bcf.usc.edu/~gareth/ISL/)\n",
    "\n",
    "![gradient decent](../img/2/gradient_decent.gif)\n",
    "source: [Towards Data Science](https://towardsdatascience.com/machine-learning-fundamentals-via-linear-regression-41a5d11f5220)\n",
    "\n",
    "\n",
    "\n",
    "**Gradient Decent** is an algorithm that is used to *go down the slope* to find the minimal cost.\n",
    "When using a quadratic cost function (RSS or MSE), we can be sure there is just a single minimum.\n",
    "\n",
    "\n",
    "Linear regression is called **linear**, not because we are fitting a line, but because with gradient decent we can **linearly progress** towards the minimal cost.\n",
    "\n",
    "**R-squared**: an unit-independant meassure of model quality\n",
    "\n",
    "$$ R^2 = 1 - \\frac{\\sum_i (\\hat{y}_i - y_i)^2}{\\sum_i (\\hat{y}_i - \\mu)^2}$$\n",
    "\n",
    "<div style=\"font-size:130%;padding:25px; margin:25px; background:#ffff81; border:#000000 2px dashed;\">\n",
    "    $R^2 $: The coefficient of determination, pronounced \"R squared\", is the proportion of the variance in the dependent variable that is predictable from the independent variables.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Multiple Linear Regression<a name=\"2_1_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple linear regression attempts to model the relationship between **two or more explanatory variables** and a response variable by fitting a linear equation to observed data. \n",
    "\n",
    "$$ y = \\beta_{0}  + \\beta_{1} x_{1} + \\beta_{2} x_{2} + ... +  \\beta_{n} x_{n} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By estimating coefficients, we can find out wich of the variables influence the outcome and how.\n",
    "\n",
    "\n",
    "<img src=\"../img/2/mult_lin_regr.png\" width=\"400\" alt=\"Multiple Linear Regression\">\n",
    "\n",
    "\n",
    "Caution: Variables that have been found to be important when looked at in isolation, might not be important when examined in context.\n",
    "\n",
    "<img src=\"../img/2/simpl_mult_lin_regr_comp.png\" width=\"600\">\n",
    "\n",
    "Explanation: Newspaper and Radio advertisement are not independant, but correlated. Newspaper advertisement only seems to be effective, because of its correlation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Classification<a name=\"2_2\"></a>\n",
    "\n",
    "- The **linear regression model** assumes that the response variable **Y is numerical**.\n",
    "- But in many situations, the response variable is instead qualitative or **categorical**\n",
    "    - eye color ∈ {blue, brown, green}\n",
    "\t- email ∈ {ham, spam}\n",
    "- Predicting **qualitative responses** is known as **classification**. \n",
    "\n",
    "\n",
    "In this unit we will learn about four different **classification methods**. \n",
    "- K-Nearest Neighbour, Logistic regression, Decision Trees, Suppor Vector Machines (SVMs)\n",
    "\n",
    "Later today you will learn about others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Logistic regression<a name=\"2_2_1\"></a>\n",
    "Caution!!! The term \"Logistic Regression\" is a misnomer. The algorithm is usually NOT used for regression, but for doing classification. It gets its name from the Logistic Function $\\sigma(t)$.\n",
    "\n",
    "<A href=\"https://en.wikipedia.org/wiki/Logistic_regression\">read more on wikipedia</A>\n",
    "\n",
    "<img src=\"../img/2/logistic_regression.png\" width=600 alt=\"logistic_regression\">\n",
    "\n",
    "source: [ISLR](http://www-bcf.usc.edu/~gareth/ISL/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use:\n",
    "\n",
    "$$  p(x) = \\beta_{0}  + \\beta_{1} x $$\n",
    "\n",
    "    \n",
    "... ,but there are some problems:\n",
    "    - negative probabilities \n",
    "    - not only values between 0 and 1\n",
    "\n",
    "- Using the **logistic function** can take care of that:\n",
    "\n",
    "$$  p(x) = \\frac{ e^{\\beta_{0}  + \\beta_{1} x }}{1 +  e^{\\beta_{0}  + \\beta_{1}x}} $$\n",
    "\n",
    "Using a threshold, we can do a classification. \n",
    "~~~\n",
    "threshold = 0.5\n",
    "if p(x)>threshold then class=\"will default\" else class=\"will not default\"\n",
    "\n",
    "p(x = 1000 USD) = 0.00576\n",
    "p(x = 2000 USD) = 0.586\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 K-Nearest Neighbours (KNN)<a name=\"2_2_2\"></a>\n",
    "K Nearest Neighbors is a flexible approach to estimate the Optimal Classifier.\n",
    "For any given X we find the k closest neighbors to X in the training data, and examine their corresponding Y.\n",
    "If the majority of the Y’s are orange we predict orange otherwise guess blue.\n",
    "The smaller that k is the more flexible the method will be.\n",
    "\n",
    "<img src=\"../img/2/optimal_classifier.png\" width=\"400\">\n",
    "A simulated data set consisting of 100 observations in each of two groups, indicated in blue and in orange. The purple dashed line represents the Bayes decision boundary. The orange background grid indicates the region in which a test observation will be assigned to the orange class, and the blue background grid indicates the region in which a test observation will be assigned to the blue class.  \n",
    "\n",
    "source: [ISLR](http://www-bcf.usc.edu/~gareth/ISL/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k Nearest Neighbors is a flexible approach to estimate the target function (also called Optimal Bayes Classifier).\n",
    "\n",
    "<img src=\"../img/2/knn_03.png\" width=400>\n",
    "\n",
    "\n",
    "- For any given X we find the k closest neighbors to X in the training data, and examine their corresponding Y.\n",
    "- If the majority of the Y’s are orange we predict orange otherwise guess blue.\n",
    "- The smaller that k is the more flexible the method will be.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"../img/2/knn_10.png\" width=\"400\">\n",
    "\n",
    "\n",
    "The black curve indicates the KNN decision boundary on the data from above, using K = 10. \n",
    "The Bayes decision boundary is shown as a purple dashed line. The KNN and Bayes decision boundaries are very similar. \n",
    "\n",
    "source: [ISLR](http://www-bcf.usc.edu/~gareth/ISL/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/2/knn_1_100.png\" width=\"500\">\n",
    "\n",
    "The smaller that k is the more flexible the method will be.\n",
    " - Left: Complexity / Flexibility is high >> variance high\n",
    " - Right: Complexity is low >> bias high\n",
    " \n",
    " \n",
    "source: [ISLR](http://www-bcf.usc.edu/~gareth/ISL/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/2/knn_train_test_error.png\" width=\"500\">\n",
    "\n",
    "- The **training error** rates keep going down as k decreases or equivalently as the flexibility increases.\n",
    "\n",
    "- However, the **test error rate** at first decreases but then starts to increase again. \n",
    "\n",
    "\n",
    "source: [ISLR](http://www-bcf.usc.edu/~gareth/ISL/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias-variance tradeoff\n",
    "<div style=\"font-size:130%;padding:25px; margin:25px; background:#ffff81; border:#000000 2px dashed;\"> \n",
    "In statistics and machine learning, the **bias–variance tradeoff** is the property of a set of predictive models whereby models with a lower bias in parameter estimation have a higher variance of the parameter estimates across samples, and vice versa. \n",
    "</div>\n",
    "The **bias–variance dilemma** or problem is the conflict in trying to simultaneously minimize these two sources of error that prevent supervised learning algorithms from generalizing beyond their training set.\n",
    "\n",
    "- The **bias** is an error from erroneous assumptions in the learning algorithm. **High bias** can cause an algorithm to miss the relevant relations between features and target outputs (**underfitting**).\n",
    "- The **variance** is an error from sensitivity to small fluctuations in the training set. **High variance** can cause an algorithm to model the random noise in the training data, rather than the intended outputs (**overfitting**).\n",
    "\n",
    "[wikipedia: Bias-variance tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/2/complexity_train_test_error.png\" width=\"400\">\n",
    "\n",
    "With increasing complexity / flexibility of the model ...\n",
    "- ... **training errors** will *always decline*.\n",
    "- .... however, **test errors** will *decline at first* (as reductions in bias dominate) but will *then increase again* (as increases in variance dominate).\n",
    "\n",
    "We must always keep this picture in mind when choosing a learning method. **More flexible/complicated is not always better!** \n",
    "\n",
    "\n",
    "source: [ISLR](http://www-bcf.usc.edu/~gareth/ISL/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Famous Iris Data Set\n",
    "The Iris flower data set or Fisher's Iris data set is a multivariate data set introduced by the British statistician and biologist Ronald Fisher in his 1936 paper.\n",
    "\n",
    "<A SRC=\"https://en.wikipedia.org/wiki/Iris_flower_data_set\">wikipedia</A>\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img alt=\"iris_sepal_petal\" src=\"../img/2/iris_sepal_petal.jpg\"  width=133>\n",
    "            <A src=\"http://blog.kaggle.com/2015/04/22/scikit-learn-video-3-machine-learning-first-steps-with-the-iris-dataset/\">blog.kaggle.com</A>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img alt=\"AI\" src=\"../img/2/iris_species.jpg\" width=440>\n",
    "            <A src=\"https://www.spataru.at/iris-dataset-svm/\">www.spataru.at/iris-dataset-svm</A>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example code\n",
    "from <A SRC=\"https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py\">scikit-learn.org</A>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets\n",
    "\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# we only take the first two features. We could avoid this ugly\n",
    "# slicing by using a two-dim dataset\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "\n",
    "h = .08  # step size in the mesh\n",
    "\n",
    "# Create color maps\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "#for weights in ['uniform', 'distance']:\n",
    "n_neighbors = 3\n",
    "weights=\"uniform\"\n",
    "for n_neighbors in [1, 5]:\n",
    "    # we create an instance of Neighbours Classifier and fit the data.\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n",
    "                edgecolor='k', s=20)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.title(\"3-Class classification (k = %i, weights = '%s')\"\n",
    "              % (n_neighbors, weights))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Decision Trees<a name=\"2_2_3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tree Based Methods involve *segmenting the predictor space* into a number of simple regions.\n",
    "- The set of splitting rules used to segment the predictor space can be summarized in a tree.\n",
    "- These types of approaches are known as **decision tree methods**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree Based Methods can be used for both\n",
    "- regression\n",
    "- classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example for a regression tree\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img alt=\"baseball salary\" src=\"../img/2/decision_tree_baseball.png\" width=500></td>\n",
    "        <td><img alt=\"baseball salary\" src=\"../img/2/decision_tree_baseball_2.png\" width=250></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Predicting **salary** (colour coded (low: blue, green; high: yellow, red)) based on two features: \n",
    "number of **hits** and number of **years** of a player playing in the league.\n",
    "\n",
    "\n",
    "source: [ISLR](http://www-bcf.usc.edu/~gareth/ISL/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantages of Decision Trees\n",
    "- very **easy to explain** to people. (even easier than linear regression)\n",
    "- more closely mirror human decision-making than do the regression and classification approaches. \n",
    "- can be **displayed graphically**, and are easily interpreted even by a non-expert. \n",
    "- can easily **handle qualitative predictors** without the need to create dummy variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disadvantages of Decision Trees\n",
    "- ... generally have **low level of predictive accuracy** compared to other regression and classification approaches. \n",
    "- ... trees can be very **non-robust**. A small change in the data can cause a large change in the final estimated tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, by **aggregating many decision trees**, using methods like **bagging, random forests, and boosting**, the predictive performance of trees can be substantially improved. (next unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to implement and display decision trees in python here is a very nice example: [sklearn decision trees](https://scikit-learn.org/stable/modules/tree.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### 2.2.4 Suppor Vector Machines (SVMs)<a name=\"2_2_4\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the separation of two classes we try to find a **hyperplane** that separates them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is a hyperplane?\n",
    "In a p-dimensional space, a hyperplane is a flat affine subspace of dimension p − 1.\n",
    "- In two dimensions, a hyperplane is a line. \n",
    "- In three dimensions, a hyperplane is a plane.\n",
    "- In p > 3 dimensions, it can be hard to visualize a hyperplane, but the notion of a (p − 1)-dimensional flat subspace still applies. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"svm_f9_2\" src=\"../img/2/svm_f9_2.png\" width=600>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Maximal Margin Classifier\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td width=500>\n",
    "            <img alt=\"svm_f9_3\" src=\"../img/2/svm_f9_3.png\" width=400>\n",
    "        <td style=\"text-align:left; font-size:30\">\n",
    "The maximal margin hyperplane is the separating hyperplane for which the margin is largest.<br><br>\n",
    "We can then classify a test observation based on which side of the maximal margin hyperplane it lies. This is known as the maximal margin classifier.<br><br>\n",
    "We hope that a classifier that has a large margin on the training data will also have a large margin on the test data, and hence will classify the test observations correctly. <br><br>\n",
    "            source: <A src=\"http://www-bcf.usc.edu/~gareth/ISL/\">ISLR</A>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Support Vector Classifier\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td width=600>\n",
    "            <img alt=\"svm_f9_5\" src=\"../img/2/svm_f9_5.png\" width=400>\n",
    "        <td style=\"text-align:left; font-size:30\">\n",
    "            An additional blue observation has been added, leading to a dramatic shift in the maximal margin hyperplane shown as a solid line. <br><br>\n",
    "            We can use a <strong>support vector classifier</strong>, sometimes called a <strong>soft margin classifier</strong> for <br><br>greater robustness to individual observations, and <br>\n",
    "            better classification of most of the training observations. \n",
    "            source: <A src=\"http://www-bcf.usc.edu/~gareth/ISL/\">ISLR</A>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### C is a tuning parameter\n",
    "<table>\n",
    "    <tr>\n",
    "        <td width=500>\n",
    "            <img alt=\"svm_f9_7\" src=\"../img/2/svm_f9_7.png\" width=450>        </td>\n",
    "        <td style=\"text-align:left; font-size:30\">\n",
    "            A support vector classifier was fit using four different values of the tuning parameter C.<br><br>             \n",
    "            The largest value of C was used in the top left panel.<br><br>              \n",
    "            When C is large, then there is a high tolerance for observations being on the wrong side of the margin, and so the margin will be large. <br><br>            \n",
    "            As C decreases, the tolerance for observations being on the wrong side of the margin decreases, and the margin narrows. <br> <br> \n",
    "            source: <A src=\"http://www-bcf.usc.edu/~gareth/ISL/\">ISLR</A>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[scikit-learn SVMs](https://scikit-learn.org/stable/modules/svm.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Exercise<a name=\"2_3\"></a>\n",
    "1. Play with the parameters in the linear regression code. Change parameters in these two lines:\n",
    "    ~~~\n",
    "    m, b, noise = 10, 0, 25\n",
    "    X, y = generate_regr(50, 0, 10, m, b, noise)\n",
    "    ~~~\n",
    "    - What happens if you increase the number of samples?\n",
    "    - What happens if you add more noise?\n",
    "\n",
    "\n",
    "2. Change the parameters in the code above and observe how the cluster predictions are influenced:\n",
    "    - n_neighbors = ...\n",
    "    - weights = \"uniform\" or \"distance\"\n",
    "\n",
    "    <A SRC=\"https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\"> sklearn.neighbors.KNeighborsClassifier</A>\n",
    "\n",
    "\n",
    "3. Think about a use case for machine learning in your department or domain:\n",
    "    - What question do I want to answer? Typically something you do manually at the moment, or is handeled ineffectively.\n",
    "    - What data set can I use to address the question?\n",
    "    - Is this question really approachable by ML?\n",
    "    - Is is unsupervised, supervised (regression, classification) ML?\n",
    "    - What kind of algorithm would I suggest? (SVM, linear regression, clustering)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Learning Material <a name=\"2_4\"></a>\n",
    "- Data School Course (Kevin Markham): **Introduction to machine learning with scikit-learn** [Jupyter notebook 6](https://github.com/justmarkham/scikit-learn-videos) and [YouTube videos](https://www.youtube.com/playlist?list=PL5-da3qGB5ICeMbQuqbbCOQWcS6OYBr5A)\n",
    "- Stanford University professors Trevor Hastie and Rob Tibshirani online course based on their textbook [free PDF](http://www-bcf.usc.edu/~gareth/ISL/), **An Introduction to Statistical Learning with Applications in R (ISLR)**. [ISLR course](https://www.r-bloggers.com/in-depth-introduction-to-machine-learning-in-15-hours-of-expert-videos/)\n",
    "\n",
    "- MIT course on **Introduction to Computational Thinking and Data Science** ([lectures 9 and 10](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-0002-introduction-to-computational-thinking-and-data-science-fall-2016/lecture-videos/))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
